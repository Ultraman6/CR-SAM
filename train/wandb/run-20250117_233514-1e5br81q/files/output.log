2025-01-17 23:35:15,621 | Epoch: [0 | 200]
2025-01-17 23:35:27,739 | Phase:train -- Batch_idx:100/391-- 1056.37 samples/sec-- Loss:2.97 -- Acc:9.20
2025-01-17 23:35:39,531 | Phase:train -- Batch_idx:200/391-- 1070.69 samples/sec-- Loss:2.64 -- Acc:9.25
2025-01-17 23:35:52,447 | Phase:train -- Batch_idx:300/391-- 1042.76 samples/sec-- Loss:2.47 -- Acc:12.08
Train Loss : 2.36935096572876, Train Accuracy : 13.728 , round:  0
wandb: WARNING Tried to log to step 0 that is less than the current step 390. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
Test Loss : 1.9339583253860473, Test Accuracy : 23.58 , round:  0
2025-01-17 23:36:09,073 | best acc:23.58
2025-01-17 23:36:09,287 | Epoch: [1 | 200]
wandb: WARNING Tried to log to step 0 that is less than the current step 390. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.
2025-01-17 23:36:23,102 | Phase:train -- Batch_idx:100/391-- 926.56 samples/sec-- Loss:1.98 -- Acc:21.55
Traceback (most recent call last):
  File "E:\Github\CR-SAM\train\sam_train.py", line 268, in <module>
    main(args)
  File "E:\Github\CR-SAM\train\sam_train.py", line 226, in main
    trainloss, trainacc = run_one_epoch('train', dataset.train, model, criterion, optimizer, args)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "E:\Github\CR-SAM\train\sam_train.py", line 95, in run_one_epoch
    for batch_idx, inp_data in enumerate(loader, 1):
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torch\utils\data\dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torch\utils\data\dataloader.py", line 673, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torch\utils\data\_utils\fetch.py", line 52, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
            ~~~~~~~~~~~~^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torchvision\datasets\cifar.py", line 119, in __getitem__
    img = self.transform(img)
          ^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torchvision\transforms\transforms.py", line 95, in __call__
    img = t(img)
          ^^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torchvision\transforms\transforms.py", line 137, in __call__
    return F.to_tensor(pic)
           ^^^^^^^^^^^^^^^^
  File "D:\anaconda3\envs\pfllib\Lib\site-packages\torchvision\transforms\functional.py", line 168, in to_tensor
    img = torch.from_numpy(np.array(pic, mode_to_nptype.get(pic.mode, np.uint8), copy=True))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
